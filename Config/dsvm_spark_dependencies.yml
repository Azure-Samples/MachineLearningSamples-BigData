# Spark configuration and packages specification. The dependencies defined in
# this file will be automatically provisioned for each run that uses Spark.

# Spark configuration values can be set through the configuration dictionary.
# Spark packages can be added through the repositories and packages lists.

# For third-party python libraries, see conda_dependencies.yml.

configuration:
  "spark.app.name": "aml-bigdata-dsvm"
  "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
  "spark.default.parallelism": 32
  "spark.executor.memory": "16G"
  "spark.executor.cores": 6
  "spark.executor.instances": 1 
  "spark.driver.memory": "8G"
  "spark.driver.cores": 2
repositories:
  - "https://mmlspark.azureedge.net/maven"
packages:
  - group: "com.microsoft.ml.spark"
    artifact: "mmlspark_2.11"
    version: "0.7.9"
  - group: "com.microsoft.sqlserver"
    artifact: "mssql-jdbc"
    version: "6.2.1.jre8"
